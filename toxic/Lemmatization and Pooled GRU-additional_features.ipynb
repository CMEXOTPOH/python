{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "stopWords = []\n",
    "for i in \"\"\"!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'\"\"\":\n",
    "    stopWords.append(i)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline\n",
    "from operator import itemgetter\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from typing import List, Dict\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '32'\n",
    "\n",
    "\n",
    "def lemmatize_all(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            yield wnl.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            yield wnl.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            yield wnl.lemmatize(word, pos='a')\n",
    "        elif tag.startswith('R'):\n",
    "            yield wnl.lemmatize(word, pos='r')\n",
    "            \n",
    "        else:\n",
    "            yield word\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = []\n",
    "for i in \"\"\"!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'\"\"\":\n",
    "    stopWords.append(i)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "lemma_dict = {}\n",
    "def lemmatizer(word):\n",
    "    try:\n",
    "        word = word.lower()\n",
    "        if word in lemma_dict:\n",
    "            return lemma_dict[word]\n",
    "        else:\n",
    "            normal_form = ps.stem(word)\n",
    "            lemma_dict[word] = normal_form\n",
    "            return normal_form\n",
    "    except:\n",
    "        return 'nonascii'\n",
    "def join_2(x,*args):\n",
    "    if len(args) == 0:\n",
    "        return x\n",
    "    else:\n",
    "        return x+\" \" +args[0]\n",
    "def transform(x):\n",
    "    try:\n",
    "        for symbol in stopWords:\n",
    "            if symbol in x:\n",
    "                x = x.replace(symbol,\" \")\n",
    "                x = x.replace('  ',\" \")\n",
    "                x = x.replace('   ',\" \")\n",
    "        x = x.split()\n",
    "        x = map(lemmatizer,x)\n",
    "        #x = map(asc,x)\n",
    "        x =  reduce(join_2,x)\n",
    "        return x\n",
    "    except:\n",
    "        return \"problem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 2) (153164, 2) (312735, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "y = train[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n",
    "train.drop(['toxic','severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)\n",
    "\n",
    "merge = pd.concat((train,test))\n",
    "\n",
    "ntrain = train.shape[0]\n",
    "print(train.shape,test.shape,merge.shape)\n",
    "\n",
    "def how_many_upper(text):\n",
    "    upper = 0\n",
    "    for char in text:\n",
    "        if char.isupper():\n",
    "            upper+=1\n",
    "    return upper\n",
    "\n",
    "def text_len(text):\n",
    "    return len(text)\n",
    "\n",
    "def count_of_stop_words(text):\n",
    "    count = 0\n",
    "    for char in text:\n",
    "        if char in stopWords:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def clean_text(text):\n",
    "    for symbol in stopWords:\n",
    "        text = text.replace(symbol,\" \")\n",
    "        text = text.lower()\n",
    "        text = text.replace('  ',\" \")\n",
    "        text = text.replace('   ',\" \")\n",
    "    return text\n",
    "\n",
    "pictures = ['.gif','.jpg','.png','.tiff']\n",
    "\n",
    "def has_picture(text):\n",
    "    has = 0\n",
    "    for _ in pictures:\n",
    "        if _ in text:\n",
    "            has = 1\n",
    "    return has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Pool(processes=32)\n",
    "merge['upper_symbols'] = p.map(how_many_upper, merge.comment_text.values)\n",
    "p.terminate()\n",
    "\n",
    "p = Pool(processes=32)\n",
    "merge['text_len'] = p.map(text_len, merge.comment_text.values)\n",
    "p.terminate()\n",
    "\n",
    "merge['percentage_of_upper'] = merge.upper_symbols/merge.text_len\n",
    "\n",
    "p = Pool(processes=32)\n",
    "merge['stop_words_count'] = p.map(count_of_stop_words, merge.comment_text.values)\n",
    "p.terminate()\n",
    "\n",
    "merge['percentage_of_stop'] = merge.stop_words_count/merge.text_len\n",
    "\n",
    "p = Pool(processes=32)\n",
    "merge['has_picture'] = p.map(has_picture, merge.comment_text.values)\n",
    "p.terminate()\n",
    "\n",
    "p = Pool(processes=32)\n",
    "merge.comment_text = p.map(transform, merge.comment_text.values)\n",
    "p.terminate()\n",
    "\n",
    "def split(text):\n",
    "    return len(text.split())\n",
    "\n",
    "p = Pool(processes=32)\n",
    "merge['number_of_words'] = p.map(split, merge.comment_text.values)\n",
    "p.terminate()\n",
    "\n",
    "words_95 = np.percentile(merge.number_of_words,95)\n",
    "\n",
    "\n",
    "merge.text_len = scale(merge.text_len)\n",
    "merge.upper_symbols = scale(merge.upper_symbols)\n",
    "merge.stop_words_count = scale(merge.stop_words_count)\n",
    "merge.number_of_words = scale(merge.number_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'crawl-300d-2M.vec'\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = list(range(len(train)))\n",
    "folds = np.array(folds) % 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(text):\n",
    "    return \" \".join(lemmatize_all(str(text)))\n",
    "train_holder = train['comment_text'].copy() \n",
    "test_holder = test['comment_text'].copy() \n",
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data lemmatization begins\n",
      "Train data lemmatization ends\n",
      "Test data lemmatization begins\n",
      "Test data lemmatization ends\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"].fillna(\"fillna\").values\n",
    "\n",
    "X_train1=[]\n",
    "X_test1=[]\n",
    "#Function call to lemmatize X_train and X_test\n",
    "print (\"Train data lemmatization begins\")\n",
    "\n",
    "    \n",
    "p = Pool(processes=32)\n",
    "train_holder = p.map(lemma, train.comment_text.values)\n",
    "p.terminate()\n",
    "X_train1 = list(train_holder)\n",
    "    \n",
    "print (\"Train data lemmatization ends\")\n",
    "print (\"Test data lemmatization begins\")\n",
    "\n",
    "    \n",
    "p = Pool(processes=32)\n",
    "test_holder = p.map(lemma, test.comment_text.values)\n",
    "p.terminate()\n",
    "X_test1 = list(test_holder)    \n",
    "\n",
    "print (\"Test data lemmatization ends\")\n",
    "\n",
    "max_features = 50000\n",
    "maxlen = 150\n",
    "embed_size = 300\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train1) + list(X_test1))\n",
    "X_train = tokenizer.texts_to_sequences(X_train1)\n",
    "X_test = tokenizer.texts_to_sequences(X_test1)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dropout, Dense, BatchNormalization,MaxPooling1D, \\\n",
    "    Activation, concatenate, GRU, Embedding, Flatten,LSTM\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping#, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ), name=\"X_train\")\n",
    "    \n",
    "    upper_symbols = Input(shape=[1], name=\"upper_symbols\")\n",
    "    text_len = Input(shape=[1], name=\"text_len\")\n",
    "    percentage_of_upper = Input(shape=[1], name=\"percentage_of_upper\")\n",
    "    stop_words_count = Input(shape=[1], \n",
    "                          name=\"stop_words_count\")\n",
    "    percentage_of_stop = Input(shape=[1], name=\"percentage_of_stop\")\n",
    "    has_picture = Input(shape=[1], name=\"has_picture\")\n",
    "    number_of_words = Input(shape=[1], name=\"number_of_words\")\n",
    "    \n",
    "    x_1 = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.2)(x_1)\n",
    "    \n",
    "    x = Conv1D(64,3,activation='relu')(x)\n",
    "    x = Conv1D(64,3,activation='relu')(x)  \n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv1D(128,3,activation='relu')(x)\n",
    "    x = Conv1D(128,3,activation='relu')(x)  \n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv1D(256,3,activation='relu')(x)\n",
    "    x = Conv1D(256,3,activation='relu')(x)  \n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv1D(512,3,activation='relu')(x)\n",
    "    x = Conv1D(512,3,activation='relu')(x)  \n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Bidirectional(GRU(128, return_sequences=True,activation='relu', dropout=0.3, recurrent_dropout=0.))(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True,activation='relu', dropout=0.3, recurrent_dropout=0.))(x)\n",
    "    \n",
    "    y = Bidirectional(GRU(128, return_sequences=True,activation='relu', dropout=0.3, recurrent_dropout=0.))(x_1)\n",
    "    y = Bidirectional(GRU(128, return_sequences=False,activation='relu', dropout=0.3, recurrent_dropout=0.))(y)\n",
    "    \n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    conc = concatenate([avg_pool, max_pool,y,\n",
    "        upper_symbols\n",
    "        , text_len\n",
    "        , percentage_of_upper\n",
    "        , stop_words_count\n",
    "        , percentage_of_stop\n",
    "        , has_picture \n",
    "        , number_of_words])\n",
    "    \n",
    "    conc = Dense(128,activation='relu')(conc)\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    conc = Dense(128,activation='relu')(conc)\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=[inp, upper_symbols,text_len,percentage_of_upper,stop_words_count,percentage_of_stop,\n",
    "                  has_picture,number_of_words], outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict_predict(df_1,df_2):\n",
    "    dict_X_train = {'X_train' : df_1 ,'upper_symbols' : df_2.upper_symbols,\n",
    "              'text_len' : df_2.text_len,'percentage_of_upper' : df_2.percentage_of_upper,\n",
    "    'stop_words_count':df_2.stop_words_count,'percentage_of_stop':df_2.percentage_of_stop,\n",
    "                'has_picture':df_2.has_picture,'number_of_words': df_2.number_of_words}\n",
    "    return dict_X_train\n",
    "\n",
    "def make_dict_train(df_1,df_2,fold):\n",
    "    dict_X_train = {'X_train' : df_1[folds != fold] ,'upper_symbols' : df_2.upper_symbols[folds != fold],\n",
    "              'text_len' : df_2.text_len[folds != fold],'percentage_of_upper' : df_2.percentage_of_upper[folds != fold],\n",
    "    'stop_words_count':df_2.stop_words_count[folds != fold],'percentage_of_stop':df_2.percentage_of_stop[folds != fold],\n",
    "                'has_picture':df_2.has_picture[folds != fold],'number_of_words': df_2.number_of_words[folds != fold]}\n",
    "    return dict_X_train\n",
    "\n",
    "def make_dict_OOF(df_1,df_2,fold):\n",
    "    dict_X_train = {'X_train' : df_1[folds == fold] ,'upper_symbols' : df_2.upper_symbols[folds == fold],\n",
    "              'text_len' : df_2.text_len[folds == fold],'percentage_of_upper' : df_2.percentage_of_upper[folds == fold],\n",
    "    'stop_words_count':df_2.stop_words_count[folds == fold],'percentage_of_stop':df_2.percentage_of_stop[folds == fold],\n",
    "                'has_picture':df_2.has_picture[folds == fold],'number_of_words': df_2.number_of_words[folds == fold]}\n",
    "    \n",
    "    return dict_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/200\n",
      "119552/119678 [============================>.] - ETA: 4s - loss: 0.0670 - acc: 0.9772 \n",
      "Epoch 00001: val_loss improved from inf to 0.05623, saving model to fold_0pooled_GRU_feat.hdf5\n",
      "119678/119678 [==============================] - 4882s 41ms/step - loss: 0.0670 - acc: 0.9772 - val_loss: 0.0562 - val_acc: 0.9795\n",
      "Epoch 2/200\n",
      "119552/119678 [============================>.] - ETA: 4s - loss: 0.0492 - acc: 0.9817 \n",
      "Epoch 00002: val_loss improved from 0.05623 to 0.05536, saving model to fold_0pooled_GRU_feat.hdf5\n",
      "119678/119678 [==============================] - 4508s 38ms/step - loss: 0.0492 - acc: 0.9817 - val_loss: 0.0554 - val_acc: 0.9809\n",
      "Epoch 3/200\n",
      "119552/119678 [============================>.] - ETA: 4s - loss: 0.0460 - acc: 0.9826 \n",
      "Epoch 00003: val_loss improved from 0.05536 to 0.04334, saving model to fold_0pooled_GRU_feat.hdf5\n",
      "119678/119678 [==============================] - 4180s 35ms/step - loss: 0.0460 - acc: 0.9826 - val_loss: 0.0433 - val_acc: 0.9839\n",
      "Epoch 4/200\n",
      "119552/119678 [============================>.] - ETA: 4s - loss: 0.0438 - acc: 0.9831 \n",
      "Epoch 00004: val_loss did not improve\n",
      "119678/119678 [==============================] - 4503s 38ms/step - loss: 0.0438 - acc: 0.9831 - val_loss: 0.0462 - val_acc: 0.9834\n",
      "Epoch 5/200\n",
      "119552/119678 [============================>.] - ETA: 4s - loss: 0.0419 - acc: 0.9837 \n",
      "Epoch 00005: val_loss improved from 0.04334 to 0.04116, saving model to fold_0pooled_GRU_feat.hdf5\n",
      "119678/119678 [==============================] - 4497s 38ms/step - loss: 0.0419 - acc: 0.9837 - val_loss: 0.0412 - val_acc: 0.9843\n",
      "Epoch 6/200\n",
      "119552/119678 [============================>.] - ETA: 4s - loss: 0.0404 - acc: 0.9841 \n",
      "Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 4166s 35ms/step - loss: 0.0404 - acc: 0.9841 - val_loss: 0.0438 - val_acc: 0.9836\n",
      "Epoch 7/200\n",
      "119552/119678 [============================>.] - ETA: 4s - loss: 0.0390 - acc: 0.9847 \n",
      "Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 4498s 38ms/step - loss: 0.0390 - acc: 0.9847 - val_loss: 0.0421 - val_acc: 0.9836\n",
      "Epoch 8/200\n",
      "119552/119678 [============================>.] - ETA: 4s - loss: 0.0375 - acc: 0.9851 \n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 4503s 38ms/step - loss: 0.0376 - acc: 0.9851 - val_loss: 0.0431 - val_acc: 0.9832\n",
      "Epoch 9/200\n",
      "119552/119678 [============================>.] - ETA: 4s - loss: 0.0347 - acc: 0.9859 \n",
      "Epoch 00009: val_loss improved from 0.04116 to 0.04051, saving model to fold_0pooled_GRU_feat.hdf5\n",
      "119678/119678 [==============================] - 4449s 37ms/step - loss: 0.0347 - acc: 0.9859 - val_loss: 0.0405 - val_acc: 0.9842\n",
      "Epoch 10/200\n",
      " 42496/119678 [=========>....................] - ETA: 1:05:14 - loss: 0.0340 - acc: 0.9862"
     ]
    }
   ],
   "source": [
    "dict_OOF_predict = {}\n",
    "dict_y_predict = {}\n",
    "\n",
    "for fold in range(4):\n",
    "    model = get_model()\n",
    "    \n",
    "    callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                               patience=4,\n",
    "                               verbose=1,\n",
    "                               min_delta=1e-4),\n",
    "                 ReduceLROnPlateau(monitor='val_loss',\n",
    "                                   factor=0.1,\n",
    "                                   patience=2,\n",
    "                                   cooldown=2,\n",
    "                                   verbose=1),\n",
    "                 ModelCheckpoint(filepath=\"fold_\"+str(fold)+\"pooled_GRU_feat.hdf5\",\n",
    "                                 save_best_only=True,\n",
    "                                 save_weights_only=True,verbose=1,monitor='val_loss')]\n",
    "\n",
    "\n",
    "    dict_X_train = make_dict_train(x_train,merge[:ntrain],fold)\n",
    "    dict_X_train_OOF = make_dict_OOF(x_train,merge[:ntrain],fold)\n",
    "    dict_y = make_dict_predict(x_test,merge[ntrain:])\n",
    "\n",
    "    \n",
    "    model.fit(dict_X_train,y_train[folds != fold],epochs=200,\n",
    "              validation_data=(dict_X_train_OOF,y_train[folds == fold]),callbacks=callbacks,batch_size=256)\n",
    "    model.load_weights(\"fold_\"+str(fold)+\"pooled_GRU_feat.hdf5\")\n",
    "    \n",
    "    predicts = model.predict(dict_y)\n",
    "    dict_y_predict[\"fold_\"+str(fold)] = predicts\n",
    "\n",
    "    predicts_OOF = model.predict(dict_X_train_OOF)\n",
    "    dict_OOF_predict[\"fold_\"+str(fold)] = predicts_OOF\n",
    "    \n",
    "    del model\n",
    "    print('fold {} is finished'.format(fold))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_0,f_1,f_2,f_3 = dict_y_predict.keys()\n",
    "sub = dict_y_predict[f_0] + dict_y_predict[f_1] + dict_y_predict[f_2] + dict_y_predict[f_3]\n",
    "sub/= 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_0 = 'fold_0'\n",
    "f_1 = 'fold_1'\n",
    "f_2 = 'fold_2'\n",
    "f_3 = 'fold_3'\n",
    "OOF_X = np.zeros_like(y_train)\n",
    "OOF_X = pd.DataFrame(OOF_X)\n",
    "\n",
    "OOF_X[folds==0] = dict_OOF_predict[f_0]\n",
    "OOF_X[folds==1] = dict_OOF_predict[f_1]\n",
    "OOF_X[folds==2] = dict_OOF_predict[f_2]\n",
    "OOF_X[folds==3] = dict_OOF_predict[f_3]\n",
    "\n",
    "pd.DataFrame(OOF_X).to_csv('OOF_toxic_pooled_GRU_50000_add_features_conv.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('sample_submission.csv')\n",
    "submit[['toxic','severe_toxic','obscene','threat','insult','identity_hate']] = sub\n",
    "submit.to_csv('toxic_pooled_GRU_50000_add_features_conv.csv',index=None)\n",
    "\n",
    "pd.DataFrame(OOF_X).to_csv('second_level/OOF_toxic_pooled_GRU_50000_add_features_conv.csv',index=None)\n",
    "submit.to_csv('second_level/toxic_pooled_GRU_50000_add_features_conv.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.986704299277824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "print(roc_auc_score(np.array(y_train),(np.array(OOF_X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9844"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
