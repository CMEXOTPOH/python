{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/n01z3/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "stopWords = []\n",
    "for i in \"\"\"!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'\"\"\":\n",
    "    stopWords.append(i)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline\n",
    "from operator import itemgetter\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from typing import List, Dict\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '32'\n",
    "\n",
    "\n",
    "def lemmatize_all(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            yield wnl.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            yield wnl.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            yield wnl.lemmatize(word, pos='a')\n",
    "        elif tag.startswith('R'):\n",
    "            yield wnl.lemmatize(word, pos='r')\n",
    "            \n",
    "        else:\n",
    "            yield word\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = []\n",
    "for i in \"\"\"!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'\"\"\":\n",
    "    stopWords.append(i)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "lemma_dict = {}\n",
    "def lemmatizer(word):\n",
    "    try:\n",
    "        word = word.lower()\n",
    "        if word in lemma_dict:\n",
    "            return lemma_dict[word]\n",
    "        else:\n",
    "            normal_form = ps.stem(word)\n",
    "            lemma_dict[word] = normal_form\n",
    "            return normal_form\n",
    "    except:\n",
    "        return 'nonascii'\n",
    "def join_2(x,*args):\n",
    "    if len(args) == 0:\n",
    "        return x\n",
    "    else:\n",
    "        return x+\" \" +args[0]\n",
    "def transform(x):\n",
    "    try:\n",
    "        for symbol in stopWords:\n",
    "            if symbol in x:\n",
    "                x = x.replace(symbol,\" \")\n",
    "                x = x.replace('  ',\" \")\n",
    "                x = x.replace('   ',\" \")\n",
    "        x = x.split()\n",
    "        x = map(lemmatizer,x)\n",
    "        #x = map(asc,x)\n",
    "        x =  reduce(join_2,x)\n",
    "        return x\n",
    "    except:\n",
    "        return \"problem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 2) (153164, 2) (312735, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "y = train[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n",
    "train.drop(['toxic','severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)\n",
    "\n",
    "merge = pd.concat((train,test))\n",
    "\n",
    "ntrain = train.shape[0]\n",
    "print(train.shape,test.shape,merge.shape)\n",
    "\n",
    "def how_many_upper(text):\n",
    "    upper = 0\n",
    "    for char in text:\n",
    "        if char.isupper():\n",
    "            upper+=1\n",
    "    return upper\n",
    "\n",
    "def text_len(text):\n",
    "    return len(text)\n",
    "\n",
    "def count_of_stop_words(text):\n",
    "    count = 0\n",
    "    for char in text:\n",
    "        if char in stopWords:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def clean_text(text):\n",
    "    for symbol in stopWords:\n",
    "        text = text.replace(symbol,\" \")\n",
    "        text = text.lower()\n",
    "        text = text.replace('  ',\" \")\n",
    "        text = text.replace('   ',\" \")\n",
    "    return text\n",
    "\n",
    "pictures = ['.gif','.jpg','.png','.tiff']\n",
    "\n",
    "def has_picture(text):\n",
    "    has = 0\n",
    "    for _ in pictures:\n",
    "        if _ in text:\n",
    "            has = 1\n",
    "    return has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Pool(processes=32)\n",
    "merge['upper_symbols'] = p.map(how_many_upper, merge.comment_text.values)\n",
    "p.terminate()\n",
    "\n",
    "p = Pool(processes=32)\n",
    "merge['text_len'] = p.map(text_len, merge.comment_text.values)\n",
    "p.terminate()\n",
    "\n",
    "merge['percentage_of_upper'] = merge.upper_symbols/merge.text_len\n",
    "\n",
    "p = Pool(processes=32)\n",
    "merge['stop_words_count'] = p.map(count_of_stop_words, merge.comment_text.values)\n",
    "p.terminate()\n",
    "\n",
    "merge['percentage_of_stop'] = merge.stop_words_count/merge.text_len\n",
    "\n",
    "p = Pool(processes=32)\n",
    "merge['has_picture'] = p.map(has_picture, merge.comment_text.values)\n",
    "p.terminate()\n",
    "\n",
    "p = Pool(processes=32)\n",
    "merge.comment_text = p.map(transform, merge.comment_text.values)\n",
    "p.terminate()\n",
    "\n",
    "def split(text):\n",
    "    return len(text.split())\n",
    "\n",
    "p = Pool(processes=32)\n",
    "merge['number_of_words'] = p.map(split, merge.comment_text.values)\n",
    "p.terminate()\n",
    "\n",
    "words_95 = np.percentile(merge.number_of_words,95)\n",
    "\n",
    "\n",
    "merge.text_len = scale(merge.text_len)\n",
    "merge.upper_symbols = scale(merge.upper_symbols)\n",
    "merge.stop_words_count = scale(merge.stop_words_count)\n",
    "merge.number_of_words = scale(merge.number_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'crawl-300d-2M.vec'\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = list(range(len(train)))\n",
    "folds = np.array(folds) % 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(text):\n",
    "    return \" \".join(lemmatize_all(str(text)))\n",
    "train_holder = train['comment_text'].copy() \n",
    "test_holder = test['comment_text'].copy() \n",
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data lemmatization begins\n",
      "Train data lemmatization ends\n",
      "Test data lemmatization begins\n",
      "Test data lemmatization ends\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"].fillna(\"fillna\").values\n",
    "\n",
    "X_train1=[]\n",
    "X_test1=[]\n",
    "#Function call to lemmatize X_train and X_test\n",
    "print (\"Train data lemmatization begins\")\n",
    "\n",
    "    \n",
    "p = Pool(processes=32)\n",
    "train_holder = p.map(lemma, train.comment_text.values)\n",
    "p.terminate()\n",
    "X_train1 = list(train_holder)\n",
    "    \n",
    "print (\"Train data lemmatization ends\")\n",
    "print (\"Test data lemmatization begins\")\n",
    "\n",
    "    \n",
    "p = Pool(processes=32)\n",
    "test_holder = p.map(lemma, test.comment_text.values)\n",
    "p.terminate()\n",
    "X_test1 = list(test_holder)    \n",
    "\n",
    "print (\"Test data lemmatization ends\")\n",
    "\n",
    "max_features = 80000\n",
    "maxlen = 200\n",
    "embed_size = 300\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train1) + list(X_test1))\n",
    "X_train = tokenizer.texts_to_sequences(X_train1)\n",
    "X_test = tokenizer.texts_to_sequences(X_test1)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('str').tolist()\n",
    "x_test = x_test.astype('str').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "tfidf.fit(x_train + x_test)\n",
    "dictionary = embeddings_index\n",
    "max_idf = max(tfidf.idf_)\n",
    "word2weight = defaultdict(lambda: max_idf,[(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "def transform(X): # усрденям слова в предложении\n",
    "    \n",
    "    dim = len(next(iter(dictionary.values())))\n",
    "    return np.mean(np.array([\n",
    "        np.mean([dictionary[w]*word2weight[w] for w in words if w in dictionary] \n",
    "                or [np.zeros(dim)], axis=0)\n",
    "        for words in X\n",
    "    ]),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p = Pool(processes=12)\n",
    "x_train = p.map(transform, x_train )\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p = Pool(processes=12)\n",
    "x_test = p.map(transform, x_test )\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dropout, Dense, BatchNormalization,MaxPooling1D, \\\n",
    "    Activation, concatenate, GRU, Embedding, Flatten,LSTM,Reshape\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping#, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.advanced_activations import ELU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(x_train[0].shape[0]+7, ))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    conc = Dense(1500,activation='relu')(inp)\n",
    "    conc = Dense(1500,activation='relu')(conc)\n",
    "    conc = Dense(300)(conc)\n",
    "    conc = Dense(1500,activation='relu')(conc)\n",
    "    conc = Dense(1500,activation='relu')(conc)\n",
    "    \n",
    "    outp = Dense(x_train[0].shape[0]+7)(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, \n",
    "        outputs=outp)\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['mse'])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict_predict(df_1,df_2):\n",
    "    dict_X_train = pd.concat((df_1,df_2),axis=1)\n",
    "    return dict_X_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 312735 samples, validate on 312735 samples\n",
      "Epoch 1/2000\n",
      "312320/312735 [============================>.] - ETA: 0s - loss: 0.0027 - mean_squared_error: 0.0027\n",
      "Epoch 00001: val_loss improved from inf to 0.00208, saving model to autoencoder.hdf5\n",
      "312735/312735 [==============================] - 64s 205us/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 2/2000\n",
      "312576/312735 [============================>.] - ETA: 0s - loss: 0.0018 - mean_squared_error: 0.0018\n",
      "Epoch 00002: val_loss did not improve\n",
      "312735/312735 [==============================] - 61s 196us/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "Epoch 3/2000\n",
      "312320/312735 [============================>.] - ETA: 0s - loss: 0.0020 - mean_squared_error: 0.0020\n",
      "Epoch 00003: val_loss did not improve\n",
      "312735/312735 [==============================] - 61s 194us/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 4/2000\n",
      "312320/312735 [============================>.] - ETA: 0s - loss: 9.2880e-04 - mean_squared_error: 9.2880e-04\n",
      "Epoch 00004: val_loss improved from 0.00208 to 0.00063, saving model to autoencoder.hdf5\n",
      "312735/312735 [==============================] - 60s 192us/step - loss: 9.2786e-04 - mean_squared_error: 9.2786e-04 - val_loss: 6.2751e-04 - val_mean_squared_error: 6.2751e-04\n",
      "Epoch 5/2000\n",
      "312576/312735 [============================>.] - ETA: 0s - loss: 0.0019 - mean_squared_error: 0.0019\n",
      "Epoch 00005: val_loss did not improve\n",
      "312735/312735 [==============================] - 61s 195us/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 6/2000\n",
      "312320/312735 [============================>.] - ETA: 0s - loss: 9.0388e-04 - mean_squared_error: 9.0388e-04\n",
      "Epoch 00006: val_loss did not improve\n",
      "312735/312735 [==============================] - 61s 195us/step - loss: 9.0307e-04 - mean_squared_error: 9.0307e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 7/2000\n",
      "312576/312735 [============================>.] - ETA: 0s - loss: 8.3087e-04 - mean_squared_error: 8.3087e-04\n",
      "Epoch 00007: val_loss improved from 0.00063 to 0.00013, saving model to autoencoder.hdf5\n",
      "312735/312735 [==============================] - 60s 193us/step - loss: 8.3050e-04 - mean_squared_error: 8.3050e-04 - val_loss: 1.3018e-04 - val_mean_squared_error: 1.3018e-04\n",
      "Epoch 8/2000\n",
      "312320/312735 [============================>.] - ETA: 0s - loss: 3.4312e-04 - mean_squared_error: 3.4312e-04\n",
      "Epoch 00008: val_loss improved from 0.00013 to 0.00012, saving model to autoencoder.hdf5\n",
      "312735/312735 [==============================] - 60s 193us/step - loss: 3.4280e-04 - mean_squared_error: 3.4280e-04 - val_loss: 1.1842e-04 - val_mean_squared_error: 1.1842e-04\n",
      "Epoch 9/2000\n",
      "312576/312735 [============================>.] - ETA: 0s - loss: 3.3751e-04 - mean_squared_error: 3.3751e-04\n",
      "Epoch 00009: val_loss did not improve\n",
      "312735/312735 [==============================] - 61s 196us/step - loss: 3.3736e-04 - mean_squared_error: 3.3736e-04 - val_loss: 2.3572e-04 - val_mean_squared_error: 2.3572e-04\n",
      "Epoch 10/2000\n",
      "312320/312735 [============================>.] - ETA: 0s - loss: 4.3236e-04 - mean_squared_error: 4.3236e-04\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "312735/312735 [==============================] - 61s 195us/step - loss: 4.3201e-04 - mean_squared_error: 4.3201e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 11/2000\n",
      "312320/312735 [============================>.] - ETA: 0s - loss: 3.5244e-04 - mean_squared_error: 3.5244e-04\n",
      "Epoch 00011: val_loss improved from 0.00012 to 0.00009, saving model to autoencoder.hdf5\n",
      "312735/312735 [==============================] - 61s 195us/step - loss: 3.5284e-04 - mean_squared_error: 3.5284e-04 - val_loss: 8.7177e-05 - val_mean_squared_error: 8.7177e-05\n",
      "Epoch 00011: early stopping\n",
      "fold 0 is finished\n"
     ]
    }
   ],
   "source": [
    "dict_OOF_predict = {}\n",
    "dict_y_predict = {}\n",
    "\n",
    "for fold in range(1):\n",
    "    model = get_model()\n",
    "    \n",
    "    callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                               patience=4,\n",
    "                               verbose=1,\n",
    "                               min_delta=1e-4),\n",
    "                 ReduceLROnPlateau(monitor='val_loss',\n",
    "                                   factor=0.1,\n",
    "                                   patience=2,\n",
    "                                   cooldown=2,\n",
    "                                   verbose=1),\n",
    "                 ModelCheckpoint(filepath=\"autoencoder.hdf5\",\n",
    "                                 save_best_only=True,\n",
    "                                 save_weights_only=True,verbose=1,monitor='val_loss')]\n",
    "\n",
    "\n",
    "\n",
    "    dict_all = make_dict_predict(pd.concat((pd.DataFrame(x_train),pd.DataFrame(x_test))),\n",
    "                                merge[['upper_symbols','text_len','percentage_of_upper',\n",
    "       'stop_words_count','percentage_of_stop','has_picture','number_of_words']])\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    model.fit(dict_all,dict_all,epochs=2000,validation_data=(dict_all,dict_all),\n",
    "            callbacks=callbacks,batch_size=256)\n",
    "\n",
    "\n",
    "    print('fold {} is finished'.format(fold))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_2():\n",
    "    inp = Input(shape=(x_train[0].shape[0]+7, ))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    conc = Dense(1500,activation='relu')(inp)\n",
    "    conc = Dense(1500,activation='relu')(conc)\n",
    "    conc = Dense(300)(conc)\n",
    "    conc = Dense(1500,activation='relu')(conc)\n",
    "    conc = Dense(1500,activation='relu')(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, \n",
    "        outputs=conc)\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['mse'])\n",
    "\n",
    "    return model\n",
    "model_2 = get_model_2()\n",
    "model_2.set_weights(model.get_weights()[0:len(model_2.weights)])\n",
    "X_last = model_2.predict(dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_2():\n",
    "    inp = Input(shape=(x_train[0].shape[0]+7, ))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    conc = Dense(1500,activation='relu')(inp)\n",
    "    conc = Dense(1500,activation='relu')(conc)\n",
    "    conc = Dense(300)(conc)\n",
    "    conc = Dense(1500,activation='relu')(conc)\n",
    "\n",
    "    \n",
    "    model = Model(inputs=inp, \n",
    "        outputs=conc)\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['mse'])\n",
    "\n",
    "    return model\n",
    "model_2 = get_model_2()\n",
    "model_2.set_weights(model.get_weights()[0:len(model_2.weights)])\n",
    "X_last_2 = model_2.predict(dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_2():\n",
    "    inp = Input(shape=(x_train[0].shape[0]+7, ))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    conc = Dense(1500,activation='relu')(inp)\n",
    "    conc = Dense(1500,activation='relu')(conc)\n",
    "    conc = Dense(300)(conc)\n",
    "\n",
    "    \n",
    "    model = Model(inputs=inp, \n",
    "        outputs=conc)\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['mse'])\n",
    "\n",
    "    return model\n",
    "model_2 = get_model_2()\n",
    "model_2.set_weights(model.get_weights()[0:len(model_2.weights)])\n",
    "X_last_3 = model_2.predict(dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_2():\n",
    "    inp = Input(shape=(x_train[0].shape[0]+7, ))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    conc = Dense(1500,activation='relu')(inp)\n",
    "    conc = Dense(1500,activation='relu')(conc)\n",
    "\n",
    "\n",
    "    \n",
    "    model = Model(inputs=inp, \n",
    "        outputs=conc)\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['mse'])\n",
    "\n",
    "    return model\n",
    "model_2 = get_model_2()\n",
    "model_2.set_weights(model.get_weights()[0:len(model_2.weights)])\n",
    "X_last_4 = model_2.predict(dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_2():\n",
    "    inp = Input(shape=(x_train[0].shape[0]+7, ))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    conc = Dense(1500,activation='relu')(inp)\n",
    "\n",
    "\n",
    "    \n",
    "    model = Model(inputs=inp, \n",
    "        outputs=conc)\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['mse'])\n",
    "\n",
    "    return model\n",
    "model_2 = get_model_2()\n",
    "model_2.set_weights(model.get_weights()[0:len(model_2.weights)])\n",
    "X_last_5 = model_2.predict(dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_0,f_1,f_2,f_3 = dict_y_predict.keys()\n",
    "sub = dict_y_predict[f_0] + dict_y_predict[f_1] + dict_y_predict[f_2] + dict_y_predict[f_3]\n",
    "sub/= 4.0\n",
    "\n",
    "f_0 = 'fold_0'\n",
    "f_1 = 'fold_1'\n",
    "f_2 = 'fold_2'\n",
    "f_3 = 'fold_3'\n",
    "OOF_X = np.zeros_like(y_train)\n",
    "OOF_X = pd.DataFrame(OOF_X)\n",
    "\n",
    "OOF_X[folds==0] = dict_OOF_predict[f_0]\n",
    "OOF_X[folds==1] = dict_OOF_predict[f_1]\n",
    "OOF_X[folds==2] = dict_OOF_predict[f_2]\n",
    "OOF_X[folds==3] = dict_OOF_predict[f_3]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "print(roc_auc_score(np.array(y_train),(np.array(OOF_X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('sample_submission.csv')\n",
    "submit[['toxic','severe_toxic','obscene','threat','insult','identity_hate']] = sub\n",
    "\n",
    "pd.DataFrame(OOF_X).to_csv('../second_level/OOF_conv_feat_2.csv',index=None)\n",
    "submit.to_csv('../second_level/conv_feat_2.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
