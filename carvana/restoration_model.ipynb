{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.layers.advanced_activations import ELU\n",
    "weg = '/media/n01z3/storage3/datasets/carvana/'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "from keras.layers import Input, merge, concatenate, MaxPooling2D, UpSampling2D, Cropping2D, ZeroPadding2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.transform import downscale_local_mean\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv2D,MaxPool2D,MaxPool1D,Conv3D,MaxPool3D,MaxPooling2D,MaxPooling3D,Dense,Flatten,Reshape\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "import gc\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.misc import imresize\n",
    "df_mask = pd.read_csv(weg+'train_masks.csv', usecols=['img'])\n",
    "ids_train = df_mask['img'].map(lambda s: s.split('_')[0]).unique()\n",
    "from keras.metrics import binary_accuracy\n",
    "from keras.models import load_model\n",
    "imgs_idx = list(range(1, 17))\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, BatchNormalization, Activation, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, BatchNormalization, Activation, UpSampling2D\n",
    "from keras.optimizers import SGD\n",
    "import keras.backend as K\n",
    "from skimage.io import imread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_img = lambda im: cv2.imread(join(weg+'train', '{}.jpg'.format(im)))\n",
    "load_mask = lambda im: imread(join(weg+'train_masks', '{}_mask.gif'.format(im)))\n",
    "load_mask_gif = lambda im: imread(join(weg+'train_masks', '{}_mask.gif'.format(im)))[:,:,0]\n",
    "\n",
    "resize = lambda im: downscale_local_mean(im, (4,4) if im.ndim==2 else (4,4,1))\n",
    "mask_image = lambda im, mask: (im * np.expand_dims(mask, 2))\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "smooth = 1.\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from renorm import BatchRenormalization\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def get_unet_1024(input_shape=(1280, 1280, 3),\n",
    "                 num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # 512\n",
    "\n",
    "    down0a = Conv2D(16, (3, 3), padding='same')(inputs)\n",
    "    down0a = BatchRenormalization()(down0a)\n",
    "    down0a = ELU()(down0a)\n",
    "    down0a = Conv2D(16, (3, 3), padding='same')(down0a)\n",
    "    down0a = BatchRenormalization()(down0a)\n",
    "    down0a = ELU()(down0a)\n",
    "    down0a_pool = MaxPooling2D((2, 2), strides=(2, 2))(down0a)\n",
    "    # 256\n",
    "\n",
    "    down0 = Conv2D(32, (3, 3), padding='same')(down0a_pool)\n",
    "    down0 = BatchRenormalization()(down0)\n",
    "    down0 = ELU()(down0)\n",
    "    down0 = Conv2D(32, (3, 3), padding='same')(down0)\n",
    "    down0 = BatchRenormalization()(down0)\n",
    "    down0 = ELU()(down0)\n",
    "    down0_pool = MaxPooling2D((2, 2), strides=(2, 2))(down0)\n",
    "    # 128\n",
    "\n",
    "    down1 = Conv2D(64, (3, 3), padding='same')(down0_pool)\n",
    "    down1 = BatchRenormalization()(down1)\n",
    "    down1 = ELU()(down1)\n",
    "    down1 = Conv2D(64, (3, 3), padding='same')(down1)\n",
    "    down1 = BatchRenormalization()(down1)\n",
    "    down1 = ELU()(down1)\n",
    "    down1_pool = MaxPooling2D((2, 2), strides=(2, 2))(down1)\n",
    "    # 64\n",
    "\n",
    "    down2 = Conv2D(128, (3, 3), padding='same')(down1_pool)\n",
    "    down2 = BatchRenormalization()(down2)\n",
    "    down2 = ELU()(down2)\n",
    "    down2 = Conv2D(128, (3, 3), padding='same')(down2)\n",
    "    down2 = BatchRenormalization()(down2)\n",
    "    down2 = ELU()(down2)\n",
    "    down2_pool = MaxPooling2D((2, 2), strides=(2, 2))(down2)\n",
    "    # 32\n",
    "\n",
    "    down3 = Conv2D(256, (3, 3), padding='same')(down2_pool)\n",
    "    down3 = BatchRenormalization()(down3)\n",
    "    down3 = ELU()(down3)\n",
    "    down3 = Conv2D(256, (3, 3), padding='same')(down3)\n",
    "    down3 = BatchRenormalization()(down3)\n",
    "    down3 = ELU()(down3)\n",
    "    down3_pool = MaxPooling2D((2, 2), strides=(2, 2))(down3)\n",
    "    # 16\n",
    "\n",
    "    down4 = Conv2D(512, (3, 3), padding='same')(down3_pool)\n",
    "    down4 = BatchRenormalization()(down4)\n",
    "    down4 = ELU()(down4)\n",
    "    down4 = Conv2D(512, (3, 3), padding='same')(down4)\n",
    "    down4 = BatchRenormalization()(down4)\n",
    "    down4 = ELU()(down4)\n",
    "    down4_pool = MaxPooling2D((2, 2), strides=(2, 2))(down4)\n",
    "    # 8\n",
    "\n",
    "    center = Conv2D(1024, (3, 3), padding='same')(down4_pool)\n",
    "    center = BatchRenormalization()(center)\n",
    "    center = ELU()(center)\n",
    "    center = Conv2D(1024, (3, 3), padding='same')(center)\n",
    "    center = BatchRenormalization()(center)\n",
    "    center = ELU()(center)\n",
    "    # center\n",
    "\n",
    "    up4 = UpSampling2D((2, 2))(center)\n",
    "    up4 = concatenate([down4, up4], axis=3)\n",
    "    up4 = Conv2D(512, (3, 3), padding='same')(up4)\n",
    "    up4 = BatchRenormalization()(up4)\n",
    "    up4 = ELU()(up4)\n",
    "    up4 = Conv2D(512, (3, 3), padding='same')(up4)\n",
    "    up4 = BatchRenormalization()(up4)\n",
    "    up4 = ELU()(up4)\n",
    "    up4 = Conv2D(512, (3, 3), padding='same')(up4)\n",
    "    up4 = BatchRenormalization()(up4)\n",
    "    up4 = ELU()(up4)\n",
    "    # 16\n",
    "\n",
    "    up3 = UpSampling2D((2, 2))(up4)\n",
    "    up3 = concatenate([down3, up3], axis=3)\n",
    "    up3 = Conv2D(256, (3, 3), padding='same')(up3)\n",
    "    up3 = BatchRenormalization()(up3)\n",
    "    up3 = ELU()(up3)\n",
    "    up3 = Conv2D(256, (3, 3), padding='same')(up3)\n",
    "    up3 = BatchRenormalization()(up3)\n",
    "    up3 = ELU()(up3)\n",
    "    up3 = Conv2D(256, (3, 3), padding='same')(up3)\n",
    "    up3 = BatchRenormalization()(up3)\n",
    "    up3 = ELU()(up3)\n",
    "    # 32\n",
    "\n",
    "    up2 = UpSampling2D((2, 2))(up3)\n",
    "    up2 = concatenate([down2, up2], axis=3)\n",
    "    up2 = Conv2D(128, (3, 3), padding='same')(up2)\n",
    "    up2 = BatchRenormalization()(up2)\n",
    "    up2 = ELU()(up2)\n",
    "    up2 = Conv2D(128, (3, 3), padding='same')(up2)\n",
    "    up2 = BatchRenormalization()(up2)\n",
    "    up2 = ELU()(up2)\n",
    "    up2 = Conv2D(128, (3, 3), padding='same')(up2)\n",
    "    up2 = BatchRenormalization()(up2)\n",
    "    up2 = ELU()(up2)\n",
    "    # 64\n",
    "\n",
    "    up1 = UpSampling2D((2, 2))(up2)\n",
    "    up1 = concatenate([down1, up1], axis=3)\n",
    "    up1 = Conv2D(64, (3, 3), padding='same')(up1)\n",
    "    up1 = BatchRenormalization()(up1)\n",
    "    up1 = ELU()(up1)\n",
    "    up1 = Conv2D(64, (3, 3), padding='same')(up1)\n",
    "    up1 = BatchRenormalization()(up1)\n",
    "    up1 = ELU()(up1)\n",
    "    up1 = Conv2D(64, (3, 3), padding='same')(up1)\n",
    "    up1 = BatchRenormalization()(up1)\n",
    "    up1 = ELU()(up1)\n",
    "    # 128\n",
    "\n",
    "    up0 = UpSampling2D((2, 2))(up1)\n",
    "    up0 = concatenate([down0, up0], axis=3)\n",
    "    up0 = Conv2D(32, (3, 3), padding='same')(up0)\n",
    "    up0 = BatchRenormalization()(up0)\n",
    "    up0 = ELU()(up0)\n",
    "    up0 = Conv2D(32, (3, 3), padding='same')(up0)\n",
    "    up0 = BatchRenormalization()(up0)\n",
    "    up0 = ELU()(up0)\n",
    "    up0 = Conv2D(32, (3, 3), padding='same')(up0)\n",
    "    up0 = BatchRenormalization()(up0)\n",
    "    up0 = ELU()(up0)\n",
    "    # 256\n",
    "\n",
    "    up0a = UpSampling2D((2, 2))(up0)\n",
    "    up0a = concatenate([down0a, up0a], axis=3)\n",
    "    up0a = Conv2D(16, (3, 3), padding='same')(up0a)\n",
    "    up0a = BatchRenormalization()(up0a)\n",
    "    up0a = ELU()(up0a)\n",
    "    up0a = Conv2D(16, (3, 3), padding='same')(up0a)\n",
    "    up0a = BatchRenormalization()(up0a)\n",
    "    up0a = ELU()(up0a)\n",
    "    up0a = Conv2D(16, (3, 3), padding='same')(up0a)\n",
    "    up0a = BatchRenormalization()(up0a)\n",
    "    up0a = ELU()(up0a)\n",
    "    # 512\n",
    "\n",
    "    classify = Conv2D(num_classes, (1, 1), activation='sigmoid')(up0a)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=classify)\n",
    "\n",
    "    model.compile(optimizer=SGD(lr=0.01, momentum=0.9), loss='binary_crossentropy', metrics=[dice_loss])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(weg+'sample_submission.csv')\n",
    "ids_test = df_test['img'].map(lambda s: s.split('.')[0])\n",
    "\n",
    "input_size = 1280\n",
    "input_size_2 = 1280\n",
    "batch_size = 1\n",
    "\n",
    "orig_width = 1918\n",
    "orig_height = 1280\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "model = get_unet_1024()\n",
    "model.load_weights(filepath='unet_new_1280_1280_2_batch_renorm_9_my_gen_ELU.hdf5')\n",
    "\n",
    "names = []\n",
    "for id in ids_test:\n",
    "    names.append('{}.jpg'.format(id))\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/stainsby/fast-tested-rle\n",
    "def run_length_encode(mask):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    inds = mask.flatten()\n",
    "    runs = np.where(inds[1:] != inds[:-1])[0] + 2\n",
    "    runs[1::2] = runs[1::2] - runs[:-1:2]\n",
    "    rle = ' '.join([str(r) for r in runs])\n",
    "    return rle\n",
    "\n",
    "\n",
    "rles = []\n",
    "\n",
    "test_splits = 8*59  # Split test set (number of splits must be multiple of 2)\n",
    "ids_test_splits = np.split(ids_test, indices_or_sections=test_splits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _mask_to_rle_string(mask):\n",
    "    \"\"\"Convert boolean/`binary uint` mask to RLE string.\"\"\"\n",
    "    # Mask to RLE\n",
    "    pixels = mask.flatten()\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] = runs[1::2] - runs[:-1:2]\n",
    "\n",
    "    # RLE to string\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, merge, concatenate, MaxPooling2D, UpSampling2D, Cropping2D, ZeroPadding2D\n",
    "import pandas as pd\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, BatchNormalization, Activation, UpSampling2D\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.transform import downscale_local_mean\n",
    "from os.path import join\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv2D,MaxPool2D,MaxPool1D,Conv3D,MaxPool3D,MaxPooling2D,MaxPooling3D,Dense,Flatten,Reshape\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from scipy.misc import imresize\n",
    "df_mask = pd.read_csv(weg+'train_masks.csv', usecols=['img'])\n",
    "ids_train = df_mask['img'].map(lambda s: s.split('_')[0]).unique()\n",
    "from keras.metrics import binary_accuracy\n",
    "imgs_idx = list(range(1, 17))\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(weg+'train_masks.csv')\n",
    "ids_train = df_train['img'].map(lambda s: s.split('.')[0])\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 1\n",
    "\n",
    "ids_train_split, ids_valid_split = train_test_split(ids_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_generator = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "mask_generator = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_gen_2(size=1,height=160,width=240,aug=False,inter=False,fit=False):\n",
    "    counter = 0 #счетчик запускам\n",
    "    global reduced\n",
    "\n",
    "\n",
    "    select = ids_train_split.values\n",
    "    number_of_batches = np.ceil(len(select)/size) #количестов партий - зависит от размера партии\n",
    "    while 1:\n",
    "\n",
    "\n",
    "\n",
    "        cc = 0\n",
    "        x = np.empty((size, height, width, 3), dtype=np.float32) # пустой массив для загрузки картинок\n",
    "        y = np.empty((size, height, width, 1), dtype=np.float32)\n",
    "        for img_id in select[size*counter:size*(counter+1)]: #16\n",
    "\n",
    "            imgs_id = [cv2.resize(load_img(img_id),(width,height))/255.]\n",
    "            # Input is image + mean image per channel + std image per channel\n",
    "            mask_16 = [cv2.resize(load_mask(img_id), ( width,height))/ 255.]\n",
    "            try:\n",
    "                if mask_16[0].shape[2]>1:\n",
    "\n",
    "                    mask_16 = [cv2.resize(load_mask_gif(img_id), ( width,height))/ 255.]\n",
    "            except:\n",
    "                pass            #X[i] = np.concatenate([imgs_id[idx-1], np.mean(imgs_id, axis=0), np.std(imgs_id, axis=0)], axis=2)\n",
    "            \n",
    "            x[cc:(1+cc)] = np.array(imgs_id)\n",
    "            y[cc:(1+cc)] = np.expand_dims(np.array(mask_16),3)\n",
    "            \n",
    "            cc+=1    \n",
    "        del imgs_id\n",
    "        counter+=1\n",
    "        SEED = int(np.random.randint(0,999999,1))\n",
    "        if inter == False: #если не чередуем данные\n",
    "            if aug == True:  # делаем ли аугментацию?\n",
    "                if fit==True:\n",
    "                    image_generator.fit(x)\n",
    "                    mask_generator.fit(y)\n",
    "\n",
    "\n",
    "                for img in image_generator.flow((x),batch_size=size,seed=SEED):\n",
    "                    break\n",
    "\n",
    "                for msk in mask_generator.flow((y),batch_size=size,seed=SEED):\n",
    "                    break \n",
    "                yield(img,msk)\n",
    "\n",
    "            if aug == False: # или базовую картинку?\n",
    "                x = np.concatenate((x,model.predict(x)),axis =3)\n",
    "                yield(x,y)\n",
    "\n",
    "        if inter == True: # если чередуем то по очереди загружаем то одно то другое\n",
    "            if counter % 2 ==0:\n",
    "                if fit==True:\n",
    "                    image_generator.fit(x)\n",
    "                    mask_generator.fit(y)\n",
    "                for img in image_generator.flow((x),batch_size=size,seed=SEED):\n",
    "                    break\n",
    "\n",
    "                for msk in mask_generator.flow((y),batch_size=size,seed=SEED):\n",
    "                    break \n",
    "                yield(img,msk)\n",
    "\n",
    "            else:\n",
    "                yield(x,y)\n",
    "                \n",
    "        if counter == number_of_batches:\n",
    "            counter = 0\n",
    "            np.random.shuffle(select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def val_gen_2(size=1,height=160,width=240,aug=False,inter = False,fit=False):\n",
    "    global reduced\n",
    "    select = ids_valid_split.values\n",
    "    number_of_batches = np.ceil(len(select)/size) #количестов партий - зависит от размера партии\n",
    "\n",
    "    counter = 0\n",
    "    while 1:\n",
    "\n",
    "\n",
    "\n",
    "        cc = 0\n",
    "        x = np.empty((size, height, width, 3), dtype=np.float32) # пустой массив для загрузки картинок\n",
    "        y = np.empty((size, height, width, 1), dtype=np.float32)\n",
    "        for img_id in select[size*counter:size*(counter+1)]: #16\n",
    "\n",
    "            imgs_id = [cv2.resize(load_img(img_id),(width,height))/255.]\n",
    "            # Input is image + mean image per channel + std image per channel\n",
    "            mask_16 = [cv2.resize(load_mask(img_id), ( width,height))/ 255.]\n",
    "            try:\n",
    "                if mask_16[0].shape[2]>1:#если не одноканальный\n",
    "\n",
    "                    mask_16 = [cv2.resize(load_mask_gif(img_id), ( width,height))/ 255.]\n",
    "            except:\n",
    "                pass            #X[i] = np.concatenate([imgs_id[idx-1], np.mean(imgs_id, axis=0), np.std(imgs_id, axis=0)], axis=2)\n",
    "            \n",
    "            x[cc:(1+cc)] = np.array(imgs_id)\n",
    "            y[cc:(1+cc)] = np.expand_dims(np.array(mask_16),3)\n",
    "            \n",
    "            cc+=1    \n",
    "        del imgs_id\n",
    "        counter+=1\n",
    "        \n",
    "        SEED = int(np.random.randint(0,999999,1))\n",
    "        if inter == False: #если не чередуем данные\n",
    "            if aug == True:  # делаем ли аугментацию?\n",
    "                if fit==True:\n",
    "                    image_generator.fit(x)\n",
    "                    mask_generator.fit(y)\n",
    "\n",
    "\n",
    "                for img in image_generator.flow((x),batch_size=size,seed=SEED):\n",
    "                    break\n",
    "                for msk in mask_generator.flow((y),batch_size=size,seed=SEED):\n",
    "                    break \n",
    "                yield(img,msk)\n",
    "                #del img,msk\n",
    "                #del x,y\n",
    "            if aug == False: # или базовую картинку?\n",
    "                x = np.concatenate((x,model.predict(x)),axis =3)\n",
    "                yield(x,y)\n",
    "                #del x,y\n",
    "        if inter == True: # если чередуем то по очереди загружаем то одно то другое\n",
    "            if counter % 2 ==0:\n",
    "                if fit==True:\n",
    "                    image_generator.fit(x)\n",
    "                    mask_generator.fit(y)\n",
    "                for img in image_generator.flow((x),batch_size=size,seed=SEED):\n",
    "                    break\n",
    "                \n",
    "                for msk in mask_generator.flow((y),batch_size=size,seed=SEED):\n",
    "                    break \n",
    "                yield(img,msk)\n",
    "                #del img,msk\n",
    "                #del x,y\n",
    "            else:\n",
    "                yield(x,y)\n",
    "                #del x,y\n",
    "        \n",
    "        if counter == number_of_batches:\n",
    "            counter = 0\n",
    "            np.random.shuffle(select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in batch_gen_2(height=1280,width=1280):\n",
    "    break\n",
    "for j in val_gen_2(height=1280,width=1280):\n",
    "    break    \n",
    "print i[0].shape\n",
    "print i[1].shape\n",
    "print j[0].shape\n",
    "print j[1].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "test = imresize(i[0].squeeze(),(1280,1918))\n",
    "plt.imshow(test, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "test = imresize(i[1].squeeze(),(1280,1918))\n",
    "plt.imshow(test, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "c = 0\n",
    "for i in batch_gen_2(height=1280,width=1920):\n",
    "    x1 = i[0].shape\n",
    "    x2 = i[1].shape\n",
    "    if c %100 ==0:\n",
    "        print c\n",
    "        print x1\n",
    "        print x2\n",
    "    c+=1\n",
    "    if c == 5000:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from renorm import BatchRenormalization\n",
    "def get_unet_1024(input_shape=(1280, 1280, 4),\n",
    "                 num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # 512\n",
    "\n",
    "    down0a = Conv2D(16, (3, 3), padding='same')(inputs)\n",
    "    down0a = BatchRenormalization()(down0a)\n",
    "    down0a = ELU()(down0a)\n",
    "    down0a = Conv2D(16, (3, 3), padding='same')(down0a)\n",
    "    down0a = BatchRenormalization()(down0a)\n",
    "    down0a = ELU()(down0a)\n",
    "    down0a_pool = MaxPooling2D((2, 2), strides=(2, 2))(down0a)\n",
    "    # 256\n",
    "\n",
    "    down0 = Conv2D(32, (3, 3), padding='same')(down0a_pool)\n",
    "    down0 = BatchRenormalization()(down0)\n",
    "    down0 = ELU()(down0)\n",
    "    down0 = Conv2D(32, (3, 3), padding='same')(down0)\n",
    "    down0 = BatchRenormalization()(down0)\n",
    "    down0 = ELU()(down0)\n",
    "    down0_pool = MaxPooling2D((2, 2), strides=(2, 2))(down0)\n",
    "    # 128\n",
    "\n",
    "    down1 = Conv2D(64, (3, 3), padding='same')(down0_pool)\n",
    "    down1 = BatchRenormalization()(down1)\n",
    "    down1 = ELU()(down1)\n",
    "    down1 = Conv2D(64, (3, 3), padding='same')(down1)\n",
    "    down1 = BatchRenormalization()(down1)\n",
    "    down1 = ELU()(down1)\n",
    "    down1_pool = MaxPooling2D((2, 2), strides=(2, 2))(down1)\n",
    "    # 64\n",
    "\n",
    "    down2 = Conv2D(128, (3, 3), padding='same')(down1_pool)\n",
    "    down2 = BatchRenormalization()(down2)\n",
    "    down2 = ELU()(down2)\n",
    "    down2 = Conv2D(128, (3, 3), padding='same')(down2)\n",
    "    down2 = BatchRenormalization()(down2)\n",
    "    down2 = ELU()(down2)\n",
    "    down2_pool = MaxPooling2D((2, 2), strides=(2, 2))(down2)\n",
    "    # 32\n",
    "\n",
    "    down3 = Conv2D(256, (3, 3), padding='same')(down2_pool)\n",
    "    down3 = BatchRenormalization()(down3)\n",
    "    down3 = ELU()(down3)\n",
    "    down3 = Conv2D(256, (3, 3), padding='same')(down3)\n",
    "    down3 = BatchRenormalization()(down3)\n",
    "    down3 = ELU()(down3)\n",
    "    down3_pool = MaxPooling2D((2, 2), strides=(2, 2))(down3)\n",
    "    # 16\n",
    "\n",
    "    down4 = Conv2D(512, (3, 3), padding='same')(down3_pool)\n",
    "    down4 = BatchRenormalization()(down4)\n",
    "    down4 = ELU()(down4)\n",
    "    down4 = Conv2D(512, (3, 3), padding='same')(down4)\n",
    "    down4 = BatchRenormalization()(down4)\n",
    "    down4 = ELU()(down4)\n",
    "    down4_pool = MaxPooling2D((2, 2), strides=(2, 2))(down4)\n",
    "    # 8\n",
    "\n",
    "    center = Conv2D(1024, (3, 3), padding='same')(down4_pool)\n",
    "    center = BatchRenormalization()(center)\n",
    "    center = ELU()(center)\n",
    "    center = Conv2D(1024, (3, 3), padding='same')(center)\n",
    "    center = BatchRenormalization()(center)\n",
    "    center = ELU()(center)\n",
    "    # center\n",
    "\n",
    "    up4 = UpSampling2D((2, 2))(center)\n",
    "    up4 = concatenate([down4, up4], axis=3)\n",
    "    up4 = Conv2D(512, (3, 3), padding='same')(up4)\n",
    "    up4 = BatchRenormalization()(up4)\n",
    "    up4 = ELU()(up4)\n",
    "    up4 = Conv2D(512, (3, 3), padding='same')(up4)\n",
    "    up4 = BatchRenormalization()(up4)\n",
    "    up4 = ELU()(up4)\n",
    "    up4 = Conv2D(512, (3, 3), padding='same')(up4)\n",
    "    up4 = BatchRenormalization()(up4)\n",
    "    up4 = ELU()(up4)\n",
    "    # 16\n",
    "\n",
    "    up3 = UpSampling2D((2, 2))(up4)\n",
    "    up3 = concatenate([down3, up3], axis=3)\n",
    "    up3 = Conv2D(256, (3, 3), padding='same')(up3)\n",
    "    up3 = BatchRenormalization()(up3)\n",
    "    up3 = ELU()(up3)\n",
    "    up3 = Conv2D(256, (3, 3), padding='same')(up3)\n",
    "    up3 = BatchRenormalization()(up3)\n",
    "    up3 = ELU()(up3)\n",
    "    up3 = Conv2D(256, (3, 3), padding='same')(up3)\n",
    "    up3 = BatchRenormalization()(up3)\n",
    "    up3 = ELU()(up3)\n",
    "    # 32\n",
    "\n",
    "    up2 = UpSampling2D((2, 2))(up3)\n",
    "    up2 = concatenate([down2, up2], axis=3)\n",
    "    up2 = Conv2D(128, (3, 3), padding='same')(up2)\n",
    "    up2 = BatchRenormalization()(up2)\n",
    "    up2 = ELU()(up2)\n",
    "    up2 = Conv2D(128, (3, 3), padding='same')(up2)\n",
    "    up2 = BatchRenormalization()(up2)\n",
    "    up2 = ELU()(up2)\n",
    "    up2 = Conv2D(128, (3, 3), padding='same')(up2)\n",
    "    up2 = BatchRenormalization()(up2)\n",
    "    up2 = ELU()(up2)\n",
    "    # 64\n",
    "\n",
    "    up1 = UpSampling2D((2, 2))(up2)\n",
    "    up1 = concatenate([down1, up1], axis=3)\n",
    "    up1 = Conv2D(64, (3, 3), padding='same')(up1)\n",
    "    up1 = BatchRenormalization()(up1)\n",
    "    up1 = ELU()(up1)\n",
    "    up1 = Conv2D(64, (3, 3), padding='same')(up1)\n",
    "    up1 = BatchRenormalization()(up1)\n",
    "    up1 = ELU()(up1)\n",
    "    up1 = Conv2D(64, (3, 3), padding='same')(up1)\n",
    "    up1 = BatchRenormalization()(up1)\n",
    "    up1 = ELU()(up1)\n",
    "    # 128\n",
    "\n",
    "    up0 = UpSampling2D((2, 2))(up1)\n",
    "    up0 = concatenate([down0, up0], axis=3)\n",
    "    up0 = Conv2D(32, (3, 3), padding='same')(up0)\n",
    "    up0 = BatchRenormalization()(up0)\n",
    "    up0 = ELU()(up0)\n",
    "    up0 = Conv2D(32, (3, 3), padding='same')(up0)\n",
    "    up0 = BatchRenormalization()(up0)\n",
    "    up0 = ELU()(up0)\n",
    "    up0 = Conv2D(32, (3, 3), padding='same')(up0)\n",
    "    up0 = BatchRenormalization()(up0)\n",
    "    up0 = ELU()(up0)\n",
    "    # 256\n",
    "\n",
    "    up0a = UpSampling2D((2, 2))(up0)\n",
    "    up0a = concatenate([down0a, up0a], axis=3)\n",
    "    up0a = Conv2D(16, (3, 3), padding='same')(up0a)\n",
    "    up0a = BatchRenormalization()(up0a)\n",
    "    up0a = ELU()(up0a)\n",
    "    up0a = Conv2D(16, (3, 3), padding='same')(up0a)\n",
    "    up0a = BatchRenormalization()(up0a)\n",
    "    up0a = ELU()(up0a)\n",
    "    up0a = Conv2D(16, (3, 3), padding='same')(up0a)\n",
    "    up0a = BatchRenormalization()(up0a)\n",
    "    up0a = ELU()(up0a)\n",
    "    # 512\n",
    "\n",
    "    classify = Conv2D(num_classes, (1, 1), activation='sigmoid')(up0a)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=classify)\n",
    "\n",
    "    model.compile(optimizer='adam', loss=dice_coef_loss, metrics=[dice_loss,'accuracy'])\n",
    "    #model.compile(optimizer=SGD(lr=0.01, momentum=0.9), loss=dice_coef_loss, metrics=[dice_loss,'accuracy'])\n",
    "    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[dice_loss,'accuracy'])\n",
    "    #model.compile(optimizer=SGD(lr=0.01, momentum=0.9), loss='binary_crossentropy', metrics=[dice_loss,'accuracy'])\n",
    "    #SGD(lr=0.01, momentum=0.9)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_rest = get_unet_1024()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='loss',\n",
    "                           patience=8,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4),\n",
    "             ReduceLROnPlateau(monitor='loss',\n",
    "                               factor=0.1,\n",
    "                               patience=4,\n",
    "                               cooldown=2,\n",
    "                               verbose=1),\n",
    "             ModelCheckpoint(filepath='restore_1280_1280_renorm_1_ELU.hdf5',monitor ='loss',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,mode = 'min',verbose=1),\n",
    "             TensorBoard(log_dir='logs')]\n",
    "             \n",
    "           \n",
    "batch_size = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_rest.fit_generator(generator=batch_gen_2(size=batch_size,height=1280,width=1280),\n",
    "        steps_per_epoch =int(len(ids_train_split)/batch_size),\n",
    "        validation_data = val_gen_2(size=1,height=1280,width=1280),\n",
    "        validation_steps=int(len(ids_valid_split)/batch_size),\n",
    "        verbose=1,callbacks=callbacks,epochs =100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rest.load_weights('restore_1280_1280_renorm_1_ELU.hdf5')\n",
    "model.load_weights(filepath='unet_new_1280_1280_2_batch_renorm_9_my_gen_ELU.hdf5')\n",
    "model_rest.evaluate_generator(generator=batch_gen_2(size=batch_size,height=1280,width=1280),steps=np.ceil(float(len(ids_train_split)) / float(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rest.evaluate_generator(generator=val_gen_2(size=1,height=1280,width=1280),steps=np.ceil(float(len(ids_train_split)) / float(batch_size)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_rest.load_weights('restore_1280_1280_renorm.hdf5')  адам - дайс лосс\n",
    "\n",
    "[-0.99736472016763333, 0.99736472016763333, 0.99836188982760288]\n",
    "[-0.99739261347187236, 0.99739261347187236, 0.99854034072644005]\n",
    "\n",
    "model_rest.load_weights('restore_1280_1280_renorm_2.hdf5')\n",
    "[-0.99734120332345333, 0.99734120332345333, 0.99854312808566359]\n",
    "\n",
    "\n",
    "model_rest.load_weights('restore_1280_1280_renorm_3.hdf5') full adam dice loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(weg+'sample_submission.csv')\n",
    "ids_test = df_test['img'].map(lambda s: s.split('.')[0])\n",
    "\n",
    "input_size = 1280\n",
    "batch_size = 1\n",
    "\n",
    "orig_width = 1918\n",
    "orig_height = 1280\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "#model = get_unet_1024()\n",
    "#model.load_weights(filepath='unet_new_1280_1280_2_batch_renorm.hdf5')\n",
    "\n",
    "names = []\n",
    "for id in ids_test:\n",
    "    names.append('{}.jpg'.format(id))\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/stainsby/fast-tested-rle\n",
    "def run_length_encode(mask):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    inds = mask.flatten()\n",
    "    runs = np.where(inds[1:] != inds[:-1])[0] + 2\n",
    "    runs[1::2] = runs[1::2] - runs[:-1:2]\n",
    "    rle = ' '.join([str(r) for r in runs])\n",
    "    return rle\n",
    "\n",
    "\n",
    "rles = []\n",
    "\n",
    "test_splits = 8*59  # Split test set (number of splits must be multiple of 2)\n",
    "ids_test_splits = np.split(ids_test, indices_or_sections=test_splits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _mask_to_rle_string(mask):\n",
    "    \"\"\"Convert boolean/`binary uint` mask to RLE string.\"\"\"\n",
    "    # Mask to RLE\n",
    "    pixels = mask.flatten()\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] = runs[1::2] - runs[:-1:2]\n",
    "\n",
    "    # RLE to string\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_count = 0\n",
    "for ids_test_split in ids_test_splits:\n",
    "    split_count += 1\n",
    "\n",
    "\n",
    "    def test_generator_2():\n",
    "        while True:\n",
    "            input_size = 1280\n",
    "            for start in range(0, len(ids_test_split), batch_size):\n",
    "                x_batch = []\n",
    "                end = min(start + batch_size, len(ids_test_split))\n",
    "                ids_test_split_batch = ids_test_split[start:end]\n",
    "                for id in ids_test_split_batch.values:\n",
    "                    img = cv2.imread(weg+'test_old/{}.jpg'.format(id))\n",
    "                    img = cv2.resize(img, (input_size, input_size))\n",
    "                    x_batch.append(img)\n",
    "                x_batch = np.array(x_batch, np.float32) / 255\n",
    "                x_batch = np.concatenate((x_batch,model.predict(x_batch)),axis =3)\n",
    "                yield x_batch\n",
    "\n",
    "\n",
    "    print(\"Predicting on {} samples (split {}/{})\".format(len(ids_test_split), split_count, test_splits))\n",
    "    preds = model_rest.predict_generator(generator=test_generator_2(),\n",
    "                                    steps=np.ceil(float(len(ids_test_split)) / float(batch_size)))\n",
    "    preds = np.squeeze(preds, axis=3)\n",
    "\n",
    "    print(\"Generating masks...\")\n",
    "    for pred in tqdm(preds, miniters=1000):\n",
    "        prob = cv2.resize(pred, (orig_width, orig_height))\n",
    "        mask = prob > threshold\n",
    "        rle = _mask_to_rle_string(mask)\n",
    "        rles.append(rle)\n",
    "\n",
    "print(\"Generating submission file...\")\n",
    "df = pd.DataFrame({'img': names, 'rle_mask': rles})\n",
    "df.to_csv('submit/restore_1280_1280_renorm_1_ELU_old_pics.csv.gz', index=False, compression='gzip')\n",
    "print 'finished'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
